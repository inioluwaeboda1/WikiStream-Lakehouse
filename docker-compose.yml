# docker-compose.yml
# Mini data stack for a streaming→lake→warehouse demo:
# - Kafka (KRaft, single broker) for ingest
# - Airflow (init/webserver/scheduler) to orchestrate Glue + Athena + RDS loads
# - Producer pushes Wikimedia RecentChanges into Kafka
# - Consumer batches NDJSON → S3 (gzip), partitioned by ds=YYYY-MM-DD

services:

  kafka:
    image: confluentinc/cp-kafka:7.8.0
    container_name: kafka
    hostname: kafka
    ports:
      - "9092:9092" # plaintext for local dev only
    environment:
      # Single-node KRaft (controller+broker in one)
      KAFKA_KRAFT_MODE: "true"
      KAFKA_PROCESS_ROLES: "broker,controller"
      KAFKA_NODE_ID: 1
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:9093"
      # Listeners/advertised for in-cluster & host access
      KAFKA_LISTENERS: "PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093"
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka:9092"
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT"
      KAFKA_INTER_BROKER_LISTENER_NAME: "PLAINTEXT"
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
      # Dev-friendly settings (single replica, auto-create topics, 7-day retention)
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_RETENTION_HOURS: 168
      # Static cluster id (required for KRaft)
      CLUSTER_ID: "d33bd245-a018-40a3-91b1-b82cf89d9e1c"
    volumes:
      - kafka-data:/var/lib/kafka/data
    restart: unless-stopped

  # ---------- Airflow (one-shot initializer) ----------
  # Idempotent DB migrate + admin user creation.
  airflow-init:
    image: apache/airflow:2.9.3
    container_name: airflow-init
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    command: >-
      bash -lc 'set -e; unset AWS_PROFILE;  # force boto3 to use env creds, not profiles airflow db migrate; airflow users create
        --username admin --password admin
        --firstname Ini --lastname Admin
        --role Admin --email inyx20@gmail.com || true'
    volumes:
      - airflow-home:/opt/airflow
    restart: "no"

  # ---------- Airflow Webserver ----------
  airflow-webserver:
    image: apache/airflow:2.9.3
    container_name: airflow-webserver
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    env_file:
      - .env # inject AWS_*, RDS_*, etc. (gitignored)
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__WEBSERVER__RBAC: 'true'
      # App settings (Athena/Glue/RDS integration)
      S3_BUCKET: "wiki-pipline-bucket"
      ATHENA_DB: "wiki_db"
      ATHENA_OUT: "s3://wiki-pipline-bucket/athena-results/"
      RDS_HOST: "${RDS_HOST}"
      RDS_DB: "${RDS_DB}"
      RDS_USER: "${RDS_USER}"
      RDS_PASSWORD: "${RDS_PASSWORD}"
      RDS_PORT: "${RDS_PORT}"
      # Airflow's aws_default uses env creds (no profile)
      AIRFLOW_CONN_AWS_DEFAULT: "aws://@?region_name=${AWS_REGION:-us-east-1}"
      # Ensure region is always set for boto3
      AWS_REGION: "${AWS_REGION:-us-east-1}"
      AWS_DEFAULT_REGION: "${AWS_REGION:-us-east-1}"
    command: >-
      bash -lc "
        set -euo pipefail;
        unset AWS_PROFILE;
        # Pin provider deps to Airflow 2.9.3 constraints for repeatable builds
        curl -fsSL https://raw.githubusercontent.com/apache/airflow/constraints-2.9.3/constraints-3.12.txt -o /tmp/constraints.txt;
        pip install --no-cache-dir -c /tmp/constraints.txt -r /opt/airflow/requirements.txt;
        exec airflow webserver
      "
    volumes:
      - airflow-home:/opt/airflow # share AIRFLOW_HOME across services
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/requirements.txt:/opt/airflow/requirements.txt
      - ./rds:/opt/airflow/rds # RDS upsert script
    ports:
      - "8080:8080"
    restart: unless-stopped

  # ---------- Airflow Scheduler ----------
  airflow-scheduler:
    image: apache/airflow:2.9.3
    container_name: airflow-scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    env_file:
      - .env # same env as webserver
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      # App settings
      S3_BUCKET: "wiki-pipline-bucket"
      ATHENA_DB: "wiki_db"
      ATHENA_OUT: "s3://wiki-pipline-bucket/athena-results/"
      RDS_HOST: "${RDS_HOST}"
      RDS_DB: "${RDS_DB}"
      RDS_USER: "${RDS_USER}"
      RDS_PASSWORD: "${RDS_PASSWORD}"
      RDS_PORT: "${RDS_PORT}"
      AIRFLOW_CONN_AWS_DEFAULT: "aws://@?region_name=${AWS_REGION:-us-east-1}"
      AWS_REGION: "${AWS_REGION:-us-east-1}"
      AWS_DEFAULT_REGION: "${AWS_REGION:-us-east-1}"
    command: >-
      bash -lc "
        set -euo pipefail;
        unset AWS_PROFILE;
        curl -fsSL https://raw.githubusercontent.com/apache/airflow/constraints-2.9.3/constraints-3.12.txt -o /tmp/constraints.txt;
        pip install --no-cache-dir -c /tmp/constraints.txt -r /opt/airflow/requirements.txt;
        exec airflow scheduler
      "
    volumes:
      - airflow-home:/opt/airflow
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/requirements.txt:/opt/airflow/requirements.txt
      - ./rds:/opt/airflow/rds
    restart: unless-stopped

  # ---------- Producer (Wikipedia → Kafka) ----------
  # Streams Wikimedia RecentChanges into Kafka (see kafka/wiki-producer.py).
  producer:
    image: python:3.11-slim
    container_name: wiki-producer
    working_dir: /app
    depends_on: [ kafka ]
    environment:
      KAFKA_BOOTSTRAP: "kafka:9092"
      USER_AGENT: "wiki-pipeline/1.0 (inioluwaeboda@example.com)" # set to your email/repo
    volumes:
      - ./kafka:/app/kafka
    command: >
      bash -lc "pip install confluent-kafka requests &&
                python -u kafka/wiki-producer.py"
    restart: unless-stopped

  # ---------- Consumer (Kafka → S3 raw NDJSON) ----------
  # Batches messages to gzip NDJSON in S3 under raw/ds=YYYY-MM-DD/.
  s3-consumer:
    image: python:3.11-slim
    container_name: wiki-s3-consumer
    working_dir: /app
    depends_on: [ kafka ]
    env_file:
      - .env # uses AWS creds + S3_BUCKET, S3_PREFIX, BATCH_SIZE, FLUSH_SECS
    environment:
      KAFKA_BOOTSTRAP: "kafka:9092"
      S3_BUCKET: "wiki-pipline-bucket"
      S3_PREFIX: "raw"
      AWS_REGION: "${AWS_REGION:-us-east-1}"
      AWS_DEFAULT_REGION: "${AWS_REGION:-us-east-1}"
      PYTHONUNBUFFERED: "1"
      BATCH_SIZE: "100" # tune batch size
      FLUSH_SECS: "10" # or time-based flush
    volumes:
      - ./kafka:/app/kafka
    command: >-
      bash -lc "unset AWS_PROFILE;
                pip install confluent-kafka boto3 &&
                python -u kafka/wiki-s3-consumer.py"
    restart: unless-stopped

volumes:
  kafka-data:
  airflow-home:
